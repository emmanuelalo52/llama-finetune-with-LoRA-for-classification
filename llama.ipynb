{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers peft datasets bitsandbytes\n",
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_datasets\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import bitsandbytes\n",
    "#List out the data set available\n",
    "class_dataset = list_datasets(full=True, filter=\"text-classification\")\n",
    "classification_dataset_names = [dataset.id for dataset in class_dataset]\n",
    "\n",
    "print(f\"There are {len(classification_dataset_names)} classification datasets available on the hub\")\n",
    "print(f\"The first 10 are: {classification_dataset_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent = load_dataset(\"ckandemir/bitcoin_tweets_sentiment_kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tweet_sent[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_sent.set_format(type=\"pandas\")\n",
    "df = tweet_sent[\"train\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "#Remove @ and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df = df.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert sentiment labels to integers\n",
    "def convert_str_int_col(df, column_name, custom_map=None):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "    if df[column_name].isnull().any():\n",
    "        raise ValueError(f\"Column '{column_name}' contains missing values.\")\n",
    "    if custom_map:\n",
    "        unique_values = df[column_name].unique()\n",
    "        for value in unique_values:\n",
    "            if value not in custom_map:\n",
    "                raise ValueError(f\"Value '{value}' in column '{column_name}' is not present in the custom_map.\")\n",
    "        df[f\"{column_name}_int\"] = df[column_name].map(custom_map)\n",
    "        mapping = custom_map\n",
    "    else:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[f\"{column_name}_int\"] = label_encoder.fit_transform(df[column_name])\n",
    "        mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    return df, mapping\n",
    "\n",
    "custom_map = {\"Positive\": 1, \"Neutral\": 0, \"Negative\": 2}\n",
    "tweet_sent_df, mapping = convert_str_int_col(tweet_sent_df, \"Sentiment\", custom_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "class FeatureCleaner(nn.Module):\n",
    "    \"\"\"\n",
    "    A feature cleaning module for text preprocessing.\n",
    "    Performs operations like URL removal, hashtag removal, stopword removal, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, slang_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureCleaner.\n",
    "\n",
    "        Args:\n",
    "            slang_dict (dict, optional): A dictionary of slang terms and their expansions.\n",
    "                                         Defaults to a predefined set of slang terms.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load stopwords\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        # Define slang dictionary (customizable)\n",
    "        self.slang_dict = slang_dict or {\n",
    "            \"HODL\": \"hold on for dear life\",\n",
    "            \"FOMO\": \"fear of missing out\",\n",
    "        }\n",
    "        # Compile regex patterns for efficiency\n",
    "        self.url_pattern = re.compile(r\"http\\S+|www\\S+|https\\S+\", re.MULTILINE)\n",
    "        self.hashtag_pattern = re.compile(r\"@\\w+|#\\w+\")\n",
    "        self.special_char_pattern = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
    "        self.date_pattern = re.compile(\n",
    "            r\"\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\d{2}/\\d{2}/\\d{4}\\b|\\b\\w{3,9}\\s\\d{1,2},?\\s\\d{4}\\b\"\n",
    "        )\n",
    "        # Compile slang replacement pattern\n",
    "        self.slang_pattern = re.compile(\"|\".join(re.escape(key) for key in self.slang_dict.keys()))\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        \"\"\"\n",
    "        Remove URLs from the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with URLs removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return self.url_pattern.sub(\"\", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        \"\"\"\n",
    "        Remove hashtags and mentions from the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with hashtags and mentions removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return self.hashtag_pattern.sub(\"\", text)\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        \"\"\"\n",
    "        Remove special characters from the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with special characters removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return self.special_char_pattern.sub(\"\", text)\n",
    "\n",
    "    def to_lowercase(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to lowercase.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Lowercase text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return text.lower()\n",
    "\n",
    "    def demoji(self, text):\n",
    "        \"\"\"\n",
    "        Convert emojis to their text representations.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with emojis converted to text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return emoji.demojize(text)\n",
    "\n",
    "    def remove_stop_words(self, text):\n",
    "        \"\"\"\n",
    "        Remove stopwords from the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with stopwords removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return \" \".join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    def expand_slangs(self, text):\n",
    "        \"\"\"\n",
    "        Expand slang terms in the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with slang terms expanded.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return self.slang_pattern.sub(lambda x: self.slang_dict[x.group()], text)\n",
    "\n",
    "    def remove_dates(self, text):\n",
    "        \"\"\"\n",
    "        Remove dates from the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with dates removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        return self.date_pattern.sub(\"\", text)\n",
    "\n",
    "    def forward(self, text, remove_stopwords=True):\n",
    "        \"\"\"\n",
    "        Apply all cleaning operations to the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "            remove_stopwords (bool, optional): Whether to remove stopwords. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "\n",
    "        # Apply cleaning operations in sequence\n",
    "        text = self.remove_url(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        text = self.demoji(text)\n",
    "        text = self.expand_slangs(text)\n",
    "        text = self.remove_dates(text)\n",
    "\n",
    "        if remove_stopwords:\n",
    "            text = self.remove_stop_words(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cleaner = FeatureCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df['text'] = tweet_sent_df['text'].apply(lambda x: feature_cleaner.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df.loc[3,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of empty or NaN rows in the 'text' column\n",
    "empty_row_count = tweet_sent_df['text'].isna().sum() + (tweet_sent_df['text'].str.strip() == \"\").sum()\n",
    "print(f\"Number of empty rows: {empty_row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty rows\n",
    "tweet_sent_df = tweet_sent_df.dropna(subset=[\"text\", \"Sentiment\", \"Sentiment_int\"])\n",
    "tweet_sent_df = tweet_sent_df[~(tweet_sent_df[\"text\"].str.strip() == \"\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df['text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test and validation datasets\n",
    "df_test = tweet_sent[\"test\"][:]\n",
    "df_val = tweet_sent[\"eval\"][:]\n",
    "\n",
    "tweet_sent_test = df_test.drop(columns=['Date'])\n",
    "tweet_sent_val = df_val.drop(columns=['Date'])\n",
    "\n",
    "tweet_sent_test, _ = convert_str_int_col(tweet_sent_test, \"Sentiment\", custom_map)\n",
    "tweet_sent_val, _ = convert_str_int_col(tweet_sent_val, \"Sentiment\", custom_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CUDA debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data types in the DataFrame\n",
    "print(\"Data types in training DataFrame:\")\n",
    "print(tweet_sent_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to datasets with explicit type conversion\n",
    "train_data = {\n",
    "    'text': tweet_sent_df['text'].tolist(),\n",
    "    'labels': tweet_sent_df['Sentiment_int'].astype(int).tolist()  # Ensure integer labels\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'text': tweet_sent_val['text'].tolist(),\n",
    "    'labels': tweet_sent_val['Sentiment_int'].astype(int).tolist()  # Ensure integer labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "llama_model = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_model)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Ensure text entries are strings\n",
    "    texts = [str(text) for text in examples['text']]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    llama_model,\n",
    "    num_labels=3,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model configuration\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for training\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"adamw_torch\",\n",
    "    no_cuda=False,\n",
    "    fp16=False,\n",
    "    bf16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
